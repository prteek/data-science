{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.5.3"
    },
    "toc": {
      "base_numbering": 1,
      "nav_menu": {},
      "number_sections": false,
      "sideBar": true,
      "skip_h1_title": false,
      "title_cell": "Table of Contents",
      "title_sidebar": "Contents",
      "toc_cell": false,
      "toc_position": {},
      "toc_section_display": true,
      "toc_window_display": false
    },
    "varInspector": {
      "cols": {
        "lenName": 16,
        "lenType": 16,
        "lenVar": 40
      },
      "kernels_config": {
        "python": {
          "delete_cmd_postfix": "",
          "delete_cmd_prefix": "del ",
          "library": "var_list.py",
          "varRefreshCmd": "print(var_dic_list())"
        },
        "r": {
          "delete_cmd_postfix": ") ",
          "delete_cmd_prefix": "rm(",
          "library": "var_list.r",
          "varRefreshCmd": "cat(var_dic_list()) "
        }
      },
      "position": {
        "height": "439px",
        "left": "1067px",
        "right": "20px",
        "top": "120px",
        "width": "353px"
      },
      "types_to_exclude": [
        "module",
        "function",
        "builtin_function_or_method",
        "instance",
        "_Feature"
      ],
      "window_display": false
    },
    "colab": {
      "name": "DecisionTrees.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/prteek/data-science/blob/master/DecisionTrees.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5uBzEFExf04H",
        "colab_type": "text"
      },
      "source": [
        "# Decision Trees\n",
        "*A tree is an incomprehensible mystery. -Jim Woodring*"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LmPZtw3wgu5Y",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# This cell is not required to be executed (i.e. ignore any error) if Notebook is run locally or in Binder\n",
        "# Authorise and mount google drive to access code and data files\n",
        "import os\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "project_folder = '/content/drive/My Drive/git_repos/data-science'\n",
        "os.chdir(project_folder)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YrbKrBADf04J",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "%%capture\n",
        "# To supress the output when calling Statistics file\n",
        "%run ./LogisticRegression.ipynb"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qp7epU2uf04T",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Entropy\n",
        "def entropy(class_probabilities):\n",
        "    \"\"\"given a list of class probabilities, compute the entropy\"\"\"\n",
        "    return sum(-p*math.log(p,2)\n",
        "              for p in class_probabilities\n",
        "              if p) # ignore zero probabilities\n",
        "\n",
        "def class_probabilities(labels):\n",
        "    total_count = len(labels)\n",
        "    return [count/total_count\n",
        "           for count in Counter(labels).values()]\n",
        "\n",
        "def data_entropy(labeled_data):\n",
        "    \"\"\"labeled_data is in the form of (input, label)\"\"\"\n",
        "    labels = [label for _, label in labeled_data]\n",
        "    probabilities = class_probabilities(labels)\n",
        "    return entropy(probabilities)\n",
        "\n",
        "# Entropy of a partition\n",
        "def partition_entropy(subsets):\n",
        "    \"\"\"find the entropy from this partition of the data into subsets\n",
        "    subsets is a list of lists of labeled data\"\"\"\n",
        "    total_count = sum(len(subset) for subset in subsets)\n",
        "    return sum(data_entropy(subset)*len(subset)/total_count\n",
        "              for subset in  subsets)\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FkMH7k0Yf04a",
        "colab_type": "text"
      },
      "source": [
        "### Creating a Decision tree"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "scrolled": false,
        "id": "u1QsVt2uf04b",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "inputs = [\n",
        "        ({'level':'Senior','lang':'Java','tweets':'no','phd':'no'},   False),\n",
        "        ({'level':'Senior','lang':'Java','tweets':'no','phd':'yes'},  False),\n",
        "        ({'level':'Mid','lang':'Python','tweets':'no','phd':'no'},     True),\n",
        "        ({'level':'Junior','lang':'Python','tweets':'no','phd':'no'},  True),\n",
        "        ({'level':'Junior','lang':'R','tweets':'yes','phd':'no'},      True),\n",
        "        ({'level':'Junior','lang':'R','tweets':'yes','phd':'yes'},    False),\n",
        "        ({'level':'Mid','lang':'R','tweets':'yes','phd':'yes'},        True),\n",
        "        ({'level':'Senior','lang':'Python','tweets':'no','phd':'no'}, False),\n",
        "        ({'level':'Senior','lang':'R','tweets':'yes','phd':'no'},      True),\n",
        "        ({'level':'Junior','lang':'Python','tweets':'yes','phd':'no'}, True),\n",
        "        ({'level':'Senior','lang':'Python','tweets':'yes','phd':'yes'},True),\n",
        "        ({'level':'Mid','lang':'Python','tweets':'no','phd':'yes'},    True),\n",
        "        ({'level':'Mid','lang':'Java','tweets':'yes','phd':'no'},      True),\n",
        "        ({'level':'Junior','lang':'Python','tweets':'no','phd':'yes'},False)\n",
        "    ]\n",
        "\n",
        "\n",
        "# The data set has both TRUE and FALSE labels, and we have 4 attributes we can split on.\n",
        "# So the first step will be to find the partition with least entropy.\n",
        "# The function below does the partitioning by given attribute\n",
        "\n",
        "def partition_by(inputs, attribute):\n",
        "    \"\"\"each input is a pair (attribute_dict, label)\n",
        "    returns a dict : attribute_value -> inputs\"\"\"\n",
        "    groups = defaultdict(list)\n",
        "    for input_i in inputs:\n",
        "        key = input_i[0][attribute]  # get the value of the specified attribute\n",
        "        groups[key].append(input_i)  # then add this input to the correct list\n",
        "    return groups\n",
        "\n",
        "# The function below uses it to compute entropy\n",
        "def partition_entropy_by(inputs, attribute, partitions_output=False):\n",
        "    \"\"\"computes the entropy corresponding to the given partition\"\"\"\n",
        "    partitions = partition_by(inputs, attribute)\n",
        "    if partitions_output:\n",
        "        return partitions, partition_entropy(partitions.values())\n",
        "    else:\n",
        "        return partition_entropy(partitions.values())\n",
        "\n",
        "# Then we can just find the minimum entropy partition for the whole dataset\n",
        "for key in ['level', 'lang', 'tweets', 'phd']:\n",
        "    parts, parts_entropy = partition_entropy_by(inputs, key, True)\n",
        "    print(key,':', round(parts_entropy,2), \n",
        "           [len(part) for part in parts.values()]) # in each attribute how many entries in each values \n",
        "\n",
        "# The lowest entropy comes from splitting on \"level\", so we need ti make a subtree for each possible level. \n",
        "# Doing this shows that every \"Mid\" candidate is labeled TRUE, which means that the \"Mid\" subtree is simply\n",
        "#Â a leaf node predicting TRUE.\n",
        "print(\"----------------\")\n",
        "\n",
        "# For \"Senior\" candidates we have a mix of TRUE and FALSE, so we need to split again.\n",
        "\n",
        "senior_inputs = [(i_input, label) for i_input, label in inputs if i_input[\"level\"]== \"Senior\"]\n",
        "\n",
        "for key in ['lang', 'tweets', 'phd']:\n",
        "    parts, parts_entropy = partition_entropy_by(senior_inputs, key, True)\n",
        "    print(key,':', round(parts_entropy,2), \n",
        "           [len(part) for part in parts.values()]) # in each attribute how many entries in each values \n",
        "\n",
        "# This shows that the next split should be on \"Tweets\", which results in a zero-entropy partition.\n",
        "# For the \"Senior\" level candidates, \"yes\" tweets always result in TRUE, while \"no\" tweets result in FALSE.\n",
        "print(\"----------------\")\n",
        "\n",
        "# Finally we do the same thing for the \"Junior\" candidates, we end up splitting on Phd, after which...\n",
        "# we find \"no\" Phd always results in TRUE and Phd always results in FALSE\n",
        "\n",
        "junior_inputs = [(i_input, label) for i_input, label in inputs if i_input[\"level\"]== \"Junior\"]\n",
        "\n",
        "for key in ['lang', 'tweets', 'phd']:\n",
        "    parts, parts_entropy = partition_entropy_by(junior_inputs, key, True)\n",
        "    print(key,':', round(parts_entropy,2), \n",
        "           [len(part) for part in parts.values()]) # in each attribute how many entries in each values "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ugVoxGsUf04h",
        "colab_type": "text"
      },
      "source": [
        "### Putting it all together\n",
        "We define a tree to be one of the following: <br/>\n",
        "**True** <br/>\n",
        "**False** <br/>\n",
        "**A tuple (attribute, subtree_dict)** <br/>\n",
        "\n",
        "*True* or *False* represents a \"leaf node\" for any input, *tuple* represents a \"decision node\" that, for any input, finds its \"attribute\" value, and classifies the input using the corresponding sub-tree. <br/>\n",
        "With this representation, our hiring tree would look like:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Hkq87GQ2f04i",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "tree = ('level', {'Junior': ('phd', {'no':True, 'yes':False}),\n",
        "                  'Mid':True,\n",
        "                  'Senior': ('tweets', {'no':False, 'yes':True})}\n",
        "       )\n",
        "\n",
        "# When we encounter an unexpected (or missing) attribute value, \n",
        "# we add a \"None\" key that just predicts the most common label. (Doesn't work if \"None\" is actually a value in data).\n",
        "\n",
        "# To classify an input using a given tree\n",
        "def classify(tree, input_i):\n",
        "    \"\"\"classify the input using the given decision tree\"\"\"\n",
        "    \n",
        "    # if this is a leaf node, return its value\n",
        "    if tree in [True, False]:\n",
        "        return tree\n",
        "    \n",
        "    # otherwise this tree consists of an attribute to split on,\n",
        "    # and a dictionary whose keys are values of that attribute\n",
        "    # and whose values are subtrees to consider next\n",
        "    \n",
        "    attribute, subtree_dict = tree\n",
        "    subtree_key             = input_i.get(attribute) # None if input is missing attribute\n",
        "    \n",
        "    if subtree_key not in subtree_dict: # if no subtree for key,\n",
        "        subtree_key = None              # we'll use None subtree\n",
        "        \n",
        "    subtree = subtree_dict[subtree_key]  # choose appropriate subtree\n",
        "    return classify(subtree, input_i)    # and use it to classify the input\n",
        "\n",
        "# Building a tree\n",
        "def build_tree_id3(inputs, split_candidates=None):\n",
        "    \n",
        "    # If this is our first pass, all the keys of the first input are split candidates\n",
        "    if split_candidates is None:\n",
        "    # inputs is a \"list of tuples\" in which first entry of pair is a dictionary of input data\n",
        "        split_candidates = inputs[0][0].keys() \n",
        "    \n",
        "    # count Trues and Falses in the inputs\n",
        "    num_inputs  = len(inputs)\n",
        "    num_trues   = len([label for item, label in inputs if label])\n",
        "    num_falses  = num_inputs - num_trues\n",
        "    \n",
        "    if num_trues == 0: return False # no Trues ? return a \"False\" leaf\n",
        "    if num_falses == 0: return True # no Falses ? return a \"True\" leaf\n",
        "    \n",
        "    if not split_candidates:            # if no split_candidates left\n",
        "        return num_trues >= num_falses  # return the majority leaf\n",
        "    \n",
        "    # otherwise, split on the best attribute\n",
        "    best_attribute = min(split_candidates,\n",
        "                        key=partial(partition_entropy_by, inputs))\n",
        "    \n",
        "    partitions = partition_by(inputs, best_attribute)\n",
        "    new_candidates = [a for a in split_candidates\n",
        "                     if a != best_attribute]\n",
        "    \n",
        "    # recursively build subtrees\n",
        "    subtrees = {attribute_value: build_tree_id3(subset, new_candidates)\n",
        "               for attribute_value, subset in partitions.items()}\n",
        "    \n",
        "    subtrees[None] = num_trues > num_falses  # default case\n",
        "    return (best_attribute, subtrees)\n",
        "        \n",
        "\n",
        "tree = build_tree_id3(inputs)\n",
        "\n",
        "print(\"Junior/Java/tweets/no phd: \", \n",
        "      classify(tree, {\"level\":\"Junior\", \"lang\":\"Java\", \"tweets\":\"yes\", \"phd\":\"no\"}))\n",
        "print(\"Junior/Java/tweets/phd: \",\n",
        "      classify(tree, {\"level\":\"Junior\", \"lang\":\"Java\", \"tweets\":\"yes\", \"phd\":\"yes\"}), \"\\n----\")\n",
        "\n",
        "# Handling missing or unexpected values of Attributes\n",
        "print(\"Intern: \", classify(tree, {\"level\":\"Intern\"}))\n",
        "print(\"Senior: \",classify(tree, {\"level\":\"Senior\"}))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Z3E2U3MQf04l",
        "colab_type": "text"
      },
      "source": [
        "### Random Forests"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xhahL7yvf04m",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def forest_classify(trees, input):\n",
        "    votes = [classify(tree, input) for tree in trees]\n",
        "    vote_counts = Counter(votes)\n",
        "    return vote_counts.most_common(1)[0][0]"
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}