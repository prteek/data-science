{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.5.3"
    },
    "varInspector": {
      "cols": {
        "lenName": 16,
        "lenType": 16,
        "lenVar": 40
      },
      "kernels_config": {
        "python": {
          "delete_cmd_postfix": "",
          "delete_cmd_prefix": "del ",
          "library": "var_list.py",
          "varRefreshCmd": "print(var_dic_list())"
        },
        "r": {
          "delete_cmd_postfix": ") ",
          "delete_cmd_prefix": "rm(",
          "library": "var_list.r",
          "varRefreshCmd": "cat(var_dic_list()) "
        }
      },
      "types_to_exclude": [
        "module",
        "function",
        "builtin_function_or_method",
        "instance",
        "_Feature"
      ],
      "window_display": false
    },
    "colab": {
      "name": "GettingData.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/prteek/data-science/blob/master/GettingData.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9QN2bJ2_Xp0S",
        "colab_type": "text"
      },
      "source": [
        "# Getting Data\n",
        "*To write it, it took three months; to concieve it, three minutes; to collect the data in it, all my life -F. Scott Fitzgerald*"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UsHLnLrDG1G6",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# This cell is not required to be executed (i.e. ignore any error) if Notebook is run locally or in Binder\n",
        "# Authorise and mount google drive to access code and data files\n",
        "\n",
        "project_folder = '/content/drive/My Drive/git_repos/data-science/'\n",
        "\n",
        "import os\n",
        "\n",
        "if os.path.isdir('/content'):\n",
        "    from google.colab import drive\n",
        "    drive.mount('/content/drive')\n",
        "\n",
        "    if not(os.path.isdir(project_folder)):\n",
        "      os.makedirs(project_folder)\n",
        "      print(\"new project folder created\")\n",
        "\n",
        "    os.chdir(project_folder)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RpFn1wwgXp0V",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "%%capture\n",
        "# To supress the output when calling another file\n",
        "%run ./GradientDescent.ipynb\n",
        "import re\n",
        "import csv"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FTnGdoMyXp0d",
        "colab_type": "text"
      },
      "source": [
        "### Reading Files\n",
        "First step is to obtain a *file object* using open:\n",
        "* 'r' means read-only\n",
        "* 'w' is write -- will destroy the file if it already exists\n",
        "* 'a' is append --for adding to the end of file\n",
        "##### Remember to close the file"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ie9MGQG1Xp0e",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "file_for_reading = open(\"requirements.txt\", \"r\")\n",
        "file_for_reading.close()\n",
        "\n",
        "# Always use a \"with\" block, which will automatically close the file at the end\n",
        "with open(\"comma_delimited_stock_prices.txt\",\"r\") as f:\n",
        "    data = [line for line in f]\n",
        "# at this point f has already been closed\n",
        "try:\n",
        "    data_again = [line for line in f]\n",
        "except:\n",
        "    print(\"ValueError: File is already closed\")\n",
        "    \n",
        "# to read a whole text file iterate over the lines of file:\n",
        "starts_with_slash =0\n",
        "with open(\"email_addresses.txt\",\"r\") as f:\n",
        "    for line in f:                     # look at each line in file\n",
        "        if re.match(\"^/\",line):        # use a regex to see if it starts with \"/\"\n",
        "            starts_with_slash +=1      # if it does, add 1 to the count\n",
        "# Note: f is closed at this point\n",
        "print(\"number of lines starting with '/': \",starts_with_slash)\n",
        "\n",
        "# every line read this way ends in a newline character and so we strip() it before doing anything with it\n",
        "\n",
        "def get_domain(email_address):\n",
        "    \"\"\"split on \"@\" and return the last piece\"\"\"\n",
        "    return email_address.lower().split(\"@\")[-1]\n",
        "\n",
        "with open(\"email_addresses.txt\",\"r\") as f:\n",
        "    list_of_domains = [get_domain(line.strip()) \n",
        "                            for line in f\n",
        "                            if \"@\" in line]\n",
        "    domain_counts   = Counter(list_of_domains)\n",
        "    \n",
        "print(\"domain counts: \", domain_counts)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tYuX1DlHXp0h",
        "colab_type": "text"
      },
      "source": [
        "### Delimited files\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "R_q9LYZ2Xp0i",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "with open(\"tab_delimited_stock_prices.txt\") as f:\n",
        "    reader = csv.reader(f, delimiter='\\t')\n",
        "    date   = list()\n",
        "    symbol = list()\n",
        "    closing_price = list()\n",
        "    for row in reader:\n",
        "        date.append(row[0])\n",
        "        symbol.append(row[1])\n",
        "        closing_price.append(float(row[2]))\n",
        "\n",
        "print(\"dates:\", date, \"symbols:\", symbol, \"closing prices\", closing_price)\n",
        "\n",
        "# if file has headers then skip header row using reader.next() or use csv.DictReader\n",
        "\n",
        "with open(\"colon_delimited_stock_prices.txt\", \"r\") as f:\n",
        "    reader = csv.DictReader(f, delimiter=\":\")\n",
        "    date   = list()\n",
        "    symbol = list()\n",
        "    closing_price = list()\n",
        "    for row in reader:\n",
        "        date.append(row[\"date\"])\n",
        "        symbol.append(row[\"symbol\"])\n",
        "        closing_price.append(float(row[\"closing_price\"]))\n",
        "\n",
        "print(\"dates:\", date, \"symbols:\", symbol, \"closing prices\", closing_price)\n",
        "\n",
        "# writing out data using csv.writer (suppressed to not change file frequently)\n",
        "\n",
        "# today_prices = {\"AAPL\":90.91, \"MSFT\":41.68, \"FB\":64.5}\n",
        "# with open(\"comma_delimited_stock_prices.txt\",\"w\") as f:\n",
        "#     writer = csv.writer(f, delimiter=\",\")\n",
        "#     for stock, price in today_prices.items():\n",
        "#         writer.writerow([stock, price])\n",
        "        \n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1NgqeKUXXp0k",
        "colab_type": "text"
      },
      "source": [
        "### Scraping the web\n",
        "#### HTML and Parsing thereof"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DHTDGK0lXp0l",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from bs4 import BeautifulSoup\n",
        "import requests\n",
        "\n",
        "# requests library makes HTTP requests better than anything that's built into Python\n",
        "# To use BeautifulSoup we'll need to pass some HTML into BeautifulSoup() function. \n",
        "# This HTML will be the result of requests.get\n",
        "\n",
        "html = requests.get(\"https://jupyter.org\").text\n",
        "soup = BeautifulSoup(html, \"html5lib\") \n",
        "# htmltlib parser is better than Python's built in one to cope with badly formatted HTML\n",
        "\n",
        "# We'll work with tag objects, which correspond to tags representing the structure of an HTML page\n",
        "first_paragraph       = soup.p # or soup.find(\"p\") p is tag for paragraph\n",
        "first_paragraph_text  = soup.p.text\n",
        "first_paragraph_words = soup.p.text.split()\n",
        "\n",
        "# and we can extract a tag's attributes by treating it like a dict:\n",
        "first_paragraph_id    = soup.p.get(\"id\") # returns None if no ID\n",
        "\n",
        "# get multiple tags at once:\n",
        "all_paragraphs        = soup(\"p\") # or just soup.find_all(\"p\")\n",
        "paragraphs_with_ids   = [p for p in all_paragraphs if p.get(\"id\")]\n",
        "\n",
        "# if we want tags with specific class:\n",
        "important_paragraphs = soup(\"p\", {\"class\":\"Notebook\"})"
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}