{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "GradientDescent.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.5.3"
    },
    "toc": {
      "base_numbering": 1,
      "nav_menu": {},
      "number_sections": false,
      "sideBar": true,
      "skip_h1_title": false,
      "title_cell": "Table of Contents",
      "title_sidebar": "Contents",
      "toc_cell": false,
      "toc_position": {},
      "toc_section_display": true,
      "toc_window_display": false
    },
    "varInspector": {
      "cols": {
        "lenName": 16,
        "lenType": 16,
        "lenVar": 40
      },
      "kernels_config": {
        "python": {
          "delete_cmd_postfix": "",
          "delete_cmd_prefix": "del ",
          "library": "var_list.py",
          "varRefreshCmd": "print(var_dic_list())"
        },
        "r": {
          "delete_cmd_postfix": ") ",
          "delete_cmd_prefix": "rm(",
          "library": "var_list.r",
          "varRefreshCmd": "cat(var_dic_list()) "
        }
      },
      "position": {
        "height": "528px",
        "left": "1066px",
        "right": "20px",
        "top": "120px",
        "width": "354px"
      },
      "types_to_exclude": [
        "module",
        "function",
        "builtin_function_or_method",
        "instance",
        "_Feature"
      ],
      "window_display": false
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/prteek/data-science/blob/master/GradientDescent.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "pnKP-YobYihp"
      },
      "source": [
        "# Gradient Descent\n",
        "*Those who brag of their descent, brag on what they owe to others. -Seneca*"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zEW-o0bPMyg4",
        "colab_type": "code",
        "outputId": "5172ec5f-ee0c-498b-f2b1-55c50899b6d3",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 122
        }
      },
      "source": [
        "# This cell is not required to be executed (i.e. ignore any error) if Notebook is run locally or in Binder\n",
        "# Authorise and mount google drive to access code and data files\n",
        "\n",
        "project_folder = '/content/drive/My Drive/git_repos/data-science/'\n",
        "\n",
        "import os\n",
        "\n",
        "if os.path.isdir('/content'):\n",
        "    from google.colab import drive\n",
        "    drive.mount('/content/drive')\n",
        "\n",
        "    if not(os.path.isdir(project_folder)):\n",
        "      os.makedirs(project_folder)\n",
        "      print(\"new project folder created\")\n",
        "\n",
        "    os.chdir(project_folder)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Go to this URL in a browser: https://accounts.google.com/o/oauth2/auth?client_id=947318989803-6bn6qk8qdgf4n4g3pfee6491hc0brc4i.apps.googleusercontent.com&redirect_uri=urn%3Aietf%3Awg%3Aoauth%3A2.0%3Aoob&scope=email%20https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fdocs.test%20https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fdrive%20https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fdrive.photos.readonly%20https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fpeopleapi.readonly&response_type=code\n",
            "\n",
            "Enter your authorization code:\n",
            "··········\n",
            "Mounted at /content/drive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "gZMuFQV1Yiht",
        "colab": {}
      },
      "source": [
        "%%capture\n",
        "# To supress the output when calling another file\n",
        "%run ./HypothesisAndInference.ipynb"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "hYcDCQa9Yih1"
      },
      "source": [
        "### Estimating the gradient\n",
        "Here's an example of estimatimg derivative of a single valued function."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "Q2-91bqeYih3",
        "outputId": "727e9eb5-427c-4673-a4bc-0af34be2008b",
        "colab": {}
      },
      "source": [
        "def difference_quotient(f,x,h=0.00001):\n",
        "    return (f(x+h) - f(x))/h\n",
        "\n",
        "def square(x):\n",
        "    return x**2\n",
        "\n",
        "def derivative_square(x):\n",
        "    return 2*x\n",
        "\n",
        "xs = range(-10,10)\n",
        "derivative_estimate = [difference_quotient(square, x, 0.00001) for x in xs]\n",
        "derivative_actual   = [derivative_square(x) for x in xs]\n",
        "\n",
        "plt.plot(xs, derivative_estimate,\"rx\",label=\"Estimate\")\n",
        "plt.plot(xs, derivative_estimate,\"b+\",label=\"Actual\")\n",
        "plt.title(\"Actual vs Estimated Derivatives for f(x) = x^2\")\n",
        "plt.xlabel('x values')\n",
        "plt.ylabel('d/dx')\n",
        "plt.legend(loc=\"upper center\")\n",
        "plt.grid()\n",
        "plt.show()"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "Jn4rTgeQYih-",
        "colab": {}
      },
      "source": [
        "def partial_difference_quotient(f, v, i, h=0.00001):\n",
        "    \"\"\"Compute the ith partial difference quotient of f at v\"\"\"\n",
        "    w = [v_j + (h if i==j else 0)  # add h to just the ith element of v\n",
        "        for j, v_j in enumerate(v)]\n",
        "    return (f(w) - f(v))/h\n",
        "\n",
        "def estimate_gradient(f, v, h=0.00001):\n",
        "    \"\"\"This is computationally expensive. If v has length n then estimate_gradient\n",
        "    has to evaluate f on 2n (f(w) and f(v)) different inputs.\"\"\"\n",
        "    return [partial_difference_quotient(f, v, i, h) for i, _ in enumerate(v)]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "e7U-RWgaYiiB"
      },
      "source": [
        "### Using the Gradient\n",
        "We'll just pick a random starting point and then take tiny steps in the opposite direction of gradient until we reach a point where the gradient is very small"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "SBD-Rb3tYiiD",
        "outputId": "addbd888-9383-49d1-efff-fbf284d3b704",
        "colab": {}
      },
      "source": [
        "def sum_of_squares_min(v): # Function to be minimised\n",
        "    return sum([v_i**2 for v_i in v])\n",
        "\n",
        "def step(v, direction, step_size):\n",
        "    \"\"\"move step_size in the direction from v\"\"\"\n",
        "    return [v_i + step_size*direction_i for v_i, direction_i in zip(v,direction)]\n",
        "\n",
        "def sum_of_squares_gradient(v): # actual gradient function\n",
        "    return [2*v_i for v_i in v]\n",
        "\n",
        "# Pick a random starting point\n",
        "v = [random.randint(-10,10) for i in range(3)]\n",
        "\n",
        "tolerance = 0.0000001\n",
        "\n",
        "while True:\n",
        "    gradient = sum_of_squares_gradient(v) # compute the gradient at v\n",
        "    next_v   = step(v, gradient, -0.01)   # take a negative gradient step\n",
        "    if distance(next_v,v) < tolerance:    # stop if we're converging\n",
        "        break\n",
        "    v = next_v                            # continue if we're not\n",
        "\n",
        "print(\"Minima with actual gradient:\", v)\n",
        "\n",
        "while True:\n",
        "    gradient = estimate_gradient(sum_of_squares, v) # compute the gradient at v\n",
        "    next_v   = step(v, gradient, -0.01)   # take a negative gradient step\n",
        "    if distance(next_v,v) < tolerance:    # stop if we're converging\n",
        "        break\n",
        "    v = next_v                            # continue if we're not\n",
        "    \n",
        "print(\"Minima with estimated gradient\", v)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Minima with actual gradient: [4.5336994434236267e-07, -2.720219666054177e-06, -4.080329499081268e-06]\n",
            "Minima with estimated gradient [-4.532650128367352e-07, -3.0992382484243446e-06, -4.233226777961894e-06]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "l3hPapRSYiiG"
      },
      "source": [
        "### Choosing the right Step Size\n",
        "Following options can be followed in selection of right step size:\n",
        "* Using a fixed step size\n",
        "* Gradually, shrinking the step size over time\n",
        "* At each step, choosing the step size that minimizes the value of the objective function (Computationally expensive)  \n",
        "\n",
        "The last option can be approximated by trying different step sizes and choosing the one that results in the smallest value of the objective function <br /> step_sizes = [100, 10, 1, 0.1, 0.01, 0.001, 0.0001, 0.00001]. <br />\n",
        "It is possible that some of the step sizes will result in invalid inputs for the function that is being minimized so a *safe apply* function is required"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "9yOhXZhwYiiI",
        "colab": {}
      },
      "source": [
        "def safe(f):\n",
        "    \"\"\"return a new function that is the same as f, \n",
        "    except that it outputs infinity whenever f produces an error\"\"\"\n",
        "    def safe_f(*args, **kwargs):\n",
        "        try:\n",
        "            return f(*args,**kwargs)\n",
        "        except:\n",
        "            return float('inf')\n",
        "    return safe_f"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "ubeAN_peYiiM"
      },
      "source": [
        "### Putting it all together\n",
        "In general we'll have some target_fn that we want to minimize and we also have its gradient_fn which is gradient w.r.t. *theta* (and not function input data). \n",
        "For example target_fn can be the sum of error in a model over the entire dataset for a given set of *theta* and we want to find the *theta* for which errors are minimum. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "ev06SenoYiiN",
        "colab": {}
      },
      "source": [
        "# It is implicit here that target_fn and gradient_fn operate over entire data set of x and y \n",
        "# and minimisation is performed w.r.t. theta\n",
        "\n",
        "def minimize_batch(target_fn, gradient_fn, theta_0, tolerance=0.0000001):\n",
        "    \"\"\"use gradient descent to find theta that minimizes target function\"\"\"\n",
        "    step_sizes = [100, 10, 1, 0.1, 0.01, 0.001, 0.0001, 0.00001]\n",
        "    target_fn  = safe(target_fn)   # safe version of target function\n",
        "    value      = target_fn(theta_0)# value we are minimising\n",
        "    next_value = value+1           # to enter while loop\n",
        "    next_theta = theta_0           # set theta to initial value\n",
        "    while abs(value - next_value)>tolerance:\n",
        "        theta                = next_theta\n",
        "        value                = next_value\n",
        "        gradient             = gradient_fn(theta)\n",
        "        next_thetas          = [step(theta, gradient, -step_size) for step_size in step_sizes]\n",
        "        # chose the theta that minimizes target_fn\n",
        "        target_fn_for_thetas = [target_fn(theta_i) for theta_i in next_thetas]\n",
        "        min_target_fn_index  = [i for i,val in enumerate(target_fn_for_thetas) \n",
        "                                if val==min(target_fn_for_thetas)]\n",
        "        next_theta           = next_thetas[min_target_fn_index[0]]\n",
        "        next_value           = target_fn(next_theta)\n",
        "\n",
        "    return theta\n",
        "\n",
        "# sometimes we'd like to maximize a function which we can do by minimising its negative\n",
        "\n",
        "def negate(f):\n",
        "    \"\"\"return a function that for any input x returns -f(x)\"\"\"\n",
        "    return lambda *args, **kwargs: -f(*args,**kwargs)\n",
        "\n",
        "def negate_all(f):\n",
        "    \"\"\"the same when f returns a list of numbers\"\"\"\n",
        "    return lambda *args, **kwargs: [-y for y in f(*args,**kwargs)]\n",
        "\n",
        "def maximize_batch(target_fn, gradient_fn, theta_0, tolerance=0.0000001):\n",
        "    return minimize_batch(negate(target_fn), negate_all(gradient_fn), theta_0, tolerance)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "GBD1Ula-YiiP"
      },
      "source": [
        "### Stochastic Gradient Descent\n",
        "Batch approach requires in each gradient step we make a prediction and compute the gradient for the whole data set, which makes each step take long time.<br/>\n",
        "\n",
        "Detailed explanation: https://www.youtube.com/watch?v=HLf4QNAwsd0\n",
        "\n",
        "Usually, the error functions are *additive* which means that the predictive error on the whole data set is simply the sum of the predictive errors for each data point. <br/> \n",
        "\n",
        "When this is the case, we can instead apply a technique called *stochastic gradient descent*, which computes the gradient (and takes a step) for only one point at a time. It cycles over our data repeatedly until it reaches a stopping point.<br/>\n",
        "\n",
        "During each cycle, we'll want to iterate through our data in random order:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "WX35ZCyWYiiP",
        "colab": {}
      },
      "source": [
        "def in_random_order(data):\n",
        "    \"\"\"generator that returns the elements of data in random order\"\"\"\n",
        "    indexes = [i for i,_ in enumerate(data)]\n",
        "    random.shuffle(indexes)\n",
        "    for i in indexes:\n",
        "        yield data[i]\n",
        "\n",
        "# This approach leaves the possibility that we might circle around near minimum forever, \n",
        "# so whenever we stop getting improvements we'll decrease step size and eventually stop:\n",
        "\n",
        "# Here only target_fn operates over entire data set but gradient_fn operates over single values of x and y\n",
        "def minimize_stochastic(target_fn, gradient_fn, x,y, theta_0, alpha_0=0.01, iterations_threshold=10):\n",
        "    data    = list(zip(x,y)) \n",
        "    theta   = theta_0    # initial guess\n",
        "    alpha   = alpha_0    # initial step size\n",
        "    min_theta, min_value = None, float(\"inf\")  # the minimum value so far\n",
        "    iterations_with_no_improvement = 0\n",
        "    \n",
        "    # if we ever go 10 iterations with no improvement, stop\n",
        "    while iterations_with_no_improvement < iterations_threshold:\n",
        "        value = sum(target_fn(x_i, y_i, theta) for x_i, y_i in data)\n",
        "        \n",
        "        if value < min_value:\n",
        "            # if we've found a new minimum, remember it\n",
        "            # and go back to the original step size\n",
        "            min_theta, min_value = theta, value\n",
        "            iterations_with_no_improvement = 0\n",
        "            alpha = alpha_0\n",
        "        else:\n",
        "            # otherwise we're not improving, so try shrinking the step size\n",
        "            iterations_with_no_improvement += 1\n",
        "            alpha *= 0.9\n",
        "            \n",
        "        # and take a gradient step for each of the data points\n",
        "        for x_i, y_i in in_random_order(data):\n",
        "            gradient_i = gradient_fn(x_i, y_i, theta)\n",
        "            theta      = vector_subtract(theta, scalar_multiply(alpha, gradient_i))\n",
        "    \n",
        "    return min_theta \n",
        "\n",
        "\n",
        "def maximize_stochastic(target_fn, gradient_fn, x,y, theta_0, alpha_0=0.01):\n",
        "    return minimize_stochastic(negate(target_fn), negate_all(gradient_fn), x, y, theta_0, alpha_0)"
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}