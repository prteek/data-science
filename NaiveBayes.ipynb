{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "0VB_R80BZYJL"
   },
   "source": [
    "# Naive Bayes\n",
    "*It is well for the heart to be naive and for the mind not to be. -Anatole France*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "NTZ_CmtMZYJN"
   },
   "outputs": [],
   "source": [
    "%%capture\n",
    "# To suppress the output when calling other files\n",
    "%run kNearestNeighbours.ipynb"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "_OPc-2HcZYJS"
   },
   "source": [
    "### A More Sophesticated Spam Filter\n",
    "In practice we want to avoid multiplying lots of probabilities together, to avoid a problem called *underflow*, in which computers don't deal well with floating point numbers that are too close to zero. Recalling from Algebra that *log(ab) = log a + log b* and that *exp(log x) = x*, we usually compute floating-point-friendlier: <br/>\n",
    "**exp(log(*p1*) + log(*p2*) + ... + log(*pn*))**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Go9Vf-exZYJT"
   },
   "outputs": [],
   "source": [
    "def tokenize(message):\n",
    "    \"\"\"Extracts \"words\" consisting of letters, numbers and apostrophes. Returns \"set\" of words as output\n",
    "    Bag of words model\"\"\"\n",
    "    message    = message.lower()                      # convert to lower case\n",
    "    all_words  = re.findall(\"[a-z0-9']+\", message)    # extract the words\n",
    "    return set(all_words)                             # set return only unique words in the message\n",
    "\n",
    "\n",
    "def count_words(training_set):\n",
    "    \"\"\"Count the words in labeled training set of messages and return a dictionary whose keys are words,\n",
    "    and whose values are two-element lists [spam_count, non_spam_count] \n",
    "    corresponding to how many times we saw that word in both spam and non-spam messages.\n",
    "    Training set consists of pairs (message, is_spam)\"\"\"\n",
    "    counts = defaultdict(lambda:[0, 0])\n",
    "    for message, is_spam in training_set: # message will contain each word only once as it is a set\n",
    "        for word in tokenize(message):\n",
    "            if is_spam:\n",
    "                counts[word][0] += 1\n",
    "            else:\n",
    "                counts[word][1] += 1\n",
    "    return counts\n",
    "\n",
    "\n",
    "def word_probabilities(counts, total_spams, total_non_spams, k=0.5):\n",
    "    \"\"\"convert the counts into estimated probabilities, using the smoothing.\n",
    "    Smoothing assumes that:\n",
    "    p(word_i/Spam) = (k + number of spams containing word_i)/(2k + number of spams)\n",
    "    i.e. when computing spam probabilities for ith word, we assume we also saw k additional spams containing\n",
    "    the word and k additional spams not containing the word.\n",
    "    Return a list of triplets w, p(w|spam) and p(w|~spam)\"\"\"\n",
    "    return [(w,\n",
    "             (spam+k)/(2*k+total_spams),\n",
    "            (non_spam+k)/(2*k+total_non_spams))\n",
    "             for w, (spam, non_spam) in counts.items()]\n",
    "\n",
    "\n",
    "def spam_probability(word_probs, message):\n",
    "    message_words          = tokenize(message)\n",
    "    log_prob_message_given_spam = log_prob_message_given_not_spam = 0.0 \n",
    "    # these will be sum of log probabilities of individual words given they appear in spam or not spam\n",
    "    \n",
    "    # Iterate through each word in the training set\n",
    "    for word, prob_wi_given_spam, prob_wi_given_not_spam in word_probs:\n",
    "        if word in message_words:\n",
    "            # if *word* appears in message, add the log probability of seeing it\n",
    "            log_prob_message_given_spam     += math.log(prob_wi_given_spam)\n",
    "            log_prob_message_given_not_spam += math.log(prob_wi_given_not_spam)\n",
    "        else:\n",
    "            # if *word* does not appear in message, add the probability of not seeing it\n",
    "            log_prob_message_given_spam     += math.log(1.0-prob_wi_given_spam)\n",
    "            log_prob_message_given_not_spam += math.log(1.0-prob_wi_given_not_spam)\n",
    "            \n",
    "    prob_message_given_spam      = math.exp(log_prob_message_given_spam)\n",
    "    prob_message_given_not_spam  = math.exp(log_prob_message_given_not_spam)\n",
    "    \n",
    "    # Assuming p(spam) = p(not_spam) = 0.5\n",
    "    prob_spam_given_message      = prob_message_given_spam/(prob_message_given_spam + prob_message_given_not_spam)\n",
    "    return prob_spam_given_message\n",
    "\n",
    "\n",
    "class NaiveBayesClassifier:\n",
    "    \n",
    "    def __init__(self, k=0.5):\n",
    "        self.k = k\n",
    "        self.word_probs = []\n",
    "        \n",
    "    def train(self, training_set):\n",
    "        \"\"\"Training set consists of pairs (message, is_spam)\"\"\"\n",
    "        # Count spam and non_spam\n",
    "        num_spams = len([is_spam\n",
    "                        for message, is_spam in training_set\n",
    "                        if is_spam])\n",
    "        num_non_spams = len(training_set) - num_spam\n",
    "        \n",
    "        # run training data through our \"pipeline\"\n",
    "        word_counts = count_words(training_set)\n",
    "        self.word_probs = word_probabilities(word_counts, num_spams, num_non_spams, self.k)\n",
    "        \n",
    "    def classify(self, message):\n",
    "        return spam_probability(self.word_probs, message)\n",
    "    "
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "include_colab_link": true,
   "name": "NaiveBayes.ipynb",
   "provenance": [],
   "version": "0.3.2"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.3"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": false,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
