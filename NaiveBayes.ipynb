{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.5.3"
    },
    "varInspector": {
      "cols": {
        "lenName": 16,
        "lenType": 16,
        "lenVar": 40
      },
      "kernels_config": {
        "python": {
          "delete_cmd_postfix": "",
          "delete_cmd_prefix": "del ",
          "library": "var_list.py",
          "varRefreshCmd": "print(var_dic_list())"
        },
        "r": {
          "delete_cmd_postfix": ") ",
          "delete_cmd_prefix": "rm(",
          "library": "var_list.r",
          "varRefreshCmd": "cat(var_dic_list()) "
        }
      },
      "types_to_exclude": [
        "module",
        "function",
        "builtin_function_or_method",
        "instance",
        "_Feature"
      ],
      "window_display": false
    },
    "colab": {
      "name": "NaiveBayes.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/prteek/data-science/blob/master/NaiveBayes.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0VB_R80BZYJL",
        "colab_type": "text"
      },
      "source": [
        "# Naive Bayes\n",
        "*It is well for the heart to be naive and for the mind not to be. -Anatole France*"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bG6HeED_Er1g",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 122
        },
        "outputId": "3a54867f-69e1-4f63-e4c9-6348d230b92e"
      },
      "source": [
        "# This cell is not required to be executed (i.e. ignore any error) if Notebook is run locally or in Binder\n",
        "# Authorise and mount google drive to access code and data files\n",
        "\n",
        "project_folder = '/content/drive/My Drive/git_repos/data-science/'\n",
        "\n",
        "import os\n",
        "\n",
        "if os.path.isdir('/content'):\n",
        "    from google.colab import drive\n",
        "    drive.mount('/content/drive')\n",
        "\n",
        "    if not(os.path.isdir(project_folder)):\n",
        "      os.makedirs(project_folder)\n",
        "      print(\"new project folder created\")\n",
        "\n",
        "    os.chdir(project_folder)"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Go to this URL in a browser: https://accounts.google.com/o/oauth2/auth?client_id=947318989803-6bn6qk8qdgf4n4g3pfee6491hc0brc4i.apps.googleusercontent.com&redirect_uri=urn%3Aietf%3Awg%3Aoauth%3A2.0%3Aoob&scope=email%20https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fdocs.test%20https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fdrive%20https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fdrive.photos.readonly%20https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fpeopleapi.readonly&response_type=code\n",
            "\n",
            "Enter your authorization code:\n",
            "··········\n",
            "Mounted at /content/drive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NTZ_CmtMZYJN",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "%%capture\n",
        "# To suppress the output when calling other files\n",
        "%run kNearestNeighbours.ipynb"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_OPc-2HcZYJS",
        "colab_type": "text"
      },
      "source": [
        "### A More Sophesticated Spam Filter\n",
        "In practice we want to avoid multiplying lots of probabilities together, to avoid a problem called *underflow*, in which computers don't deal well with floating point numbers that are too close to zero. Recalling from Algebra that *log(ab) = log a + log b* and that *exp(log x) = x*, we usually compute floating-point-friendlier: <br/>\n",
        "**exp(log(*p1*) + log(*p2*) + ... + log(*pn*))**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Go9Vf-exZYJT",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def tokenize(message):\n",
        "    \"\"\"Extracts \"words\" consisting of letters, numbers and apostrophes. Returns \"set\" of words as output\n",
        "    Bag of words model\"\"\"\n",
        "    message    = message.lower()                      # convert to lower case\n",
        "    all_words  = re.findall(\"[a-z0-9']+\", message)    # extract the words\n",
        "    return set(all_words)                             # set return only unique words in the message\n",
        "\n",
        "\n",
        "def count_words(training_set):\n",
        "    \"\"\"Count the words in labeled training set of messages and return a dictionary whose keys are words,\n",
        "    and whose values are two-element lists [spam_count, non_spam_count] \n",
        "    corresponding to how many times we saw that word in both spam and non-spam messages.\n",
        "    Training set consists of pairs (message, is_spam)\"\"\"\n",
        "    counts = defaultdict(lambda:[0, 0])\n",
        "    for message, is_spam in training_set: # message will contain each word only once as it is a set\n",
        "        for word in tokenize(message):\n",
        "            if is_spam:\n",
        "                counts[word][0] += 1\n",
        "            else:\n",
        "                counts[word][1] += 1\n",
        "    return counts\n",
        "\n",
        "\n",
        "def word_probabilities(counts, total_spams, total_non_spams, k=0.5):\n",
        "    \"\"\"convert the counts into estimated probabilities, using the smoothing.\n",
        "    Smoothing assumes that:\n",
        "    p(word_i/Spam) = (k + number of spams containing word_i)/(2k + number of spams)\n",
        "    i.e. when computing spam probabilities for ith word, we assume we also saw k additional spams containing\n",
        "    the word and k additional spams not containing the word.\n",
        "    Return a list of triplets w, p(w|spam) and p(w|~spam)\"\"\"\n",
        "    return [(w,\n",
        "             (spam+k)/(2*k+total_spams),\n",
        "            (non_spam+k)/(2*k+total_non_spams))\n",
        "             for w, (spam, non_spam) in counts.items()]\n",
        "\n",
        "\n",
        "def spam_probability(word_probs, message):\n",
        "    message_words          = tokenize(message)\n",
        "    log_prob_message_given_spam = log_prob_message_given_not_spam = 0.0 \n",
        "    # these will be sum of log probabilities of individual words given they appear in spam or not spam\n",
        "    \n",
        "    # Iterate through each word in the training set\n",
        "    for word, prob_wi_given_spam, prob_wi_given_not_spam in word_probs:\n",
        "        if word in message_words:\n",
        "            # if *word* appears in message, add the log probability of seeing it\n",
        "            log_prob_message_given_spam     += math.log(prob_wi_given_spam)\n",
        "            log_prob_message_given_not_spam += math.log(prob_wi_given_not_spam)\n",
        "        else:\n",
        "            # if *word* does not appear in message, add the probability of not seeing it\n",
        "            log_prob_message_given_spam     += math.log(1.0-prob_wi_given_spam)\n",
        "            log_prob_message_given_not_spam += math.log(1.0-prob_wi_given_not_spam)\n",
        "            \n",
        "    prob_message_given_spam      = math.exp(log_prob_message_given_spam)\n",
        "    prob_message_given_not_spam  = math.exp(log_prob_message_given_not_spam)\n",
        "    \n",
        "    # Assuming p(spam) = p(not_spam) = 0.5\n",
        "    prob_spam_given_message      = prob_message_given_spam/(prob_message_given_spam + prob_message_given_not_spam)\n",
        "    return prob_spam_given_message\n",
        "\n",
        "\n",
        "class NaiveBayesClassifier:\n",
        "    \n",
        "    def __init__(self, k=0.5):\n",
        "        self.k = k\n",
        "        self.word_probs = []\n",
        "        \n",
        "    def train(self, training_set):\n",
        "        \"\"\"Training set consists of pairs (message, is_spam)\"\"\"\n",
        "        # Count spam and non_spam\n",
        "        num_spams = len([is_spam\n",
        "                        for message, is_spam in training_set\n",
        "                        if is_spam])\n",
        "        num_non_spams = len(training_set) - num_spam\n",
        "        \n",
        "        # run training data through our \"pipeline\"\n",
        "        word_counts = count_words(training_set)\n",
        "        self.word_probs = word_probabilities(word_counts, num_spams, num_non_spams, self.k)\n",
        "        \n",
        "    def classify(self, message):\n",
        "        return spam_probability(self.word_probs, message)\n",
        "    "
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}