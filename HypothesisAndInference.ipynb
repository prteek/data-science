{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Hypothesis and Inference\n",
    "##### It is the mark of a truly intelligent person to be moved by statistics. -George Bernard Shaw"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%capture\n",
    "# To supress the output when calling another file\n",
    "%run ./Probability.ipynb"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Statistical hypothesis testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def normal_approximation_to_binomial(n,p):\n",
    "    \"\"\"finds mu and sigma corresponding to a Binomial(n,p)\"\"\"\n",
    "    mu    = n*p\n",
    "    \"\"\"variance = E(X^2)-E(X)^2 +(E(X)-E(X))\n",
    "                = E(X[X-1])+E(X)-E(X)^2 \n",
    "                = np*(p*(n-1))+np-(np)^2 \n",
    "                = np(1-p) \"\"\"\n",
    "    sigma = math.sqrt(n*p*(1-p)) \n",
    "    \n",
    "    return mu,sigma"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Whenever a random variable follows Normal distribution, normal_cdf can be used to figure out  the probability that its realized value lies within (or outside) a particular interval"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The normal_cdf is the probability the variable is below a threshold\n",
    "\n",
    "normal_probability_below = normal_cdf\n",
    "\n",
    "# it's above a threshold if it's not below the threshold\n",
    "def normal_probability_above(lo,mu=0,sigma=1):\n",
    "    return 1-normal_cdf(lo,mu,sigma)\n",
    "\n",
    "# it's between if it's less than hi, but not less than lo\n",
    "def normal_probability_between(lo,hi,mu=0,sigma=1):\n",
    "    return normal_probability_below(hi,mu,sigma) - normal_probability_below(lo,mu,sigma)\n",
    "\n",
    "# it's outside if it's not in between\n",
    "def normal_probability_outside(lo,hi,mu=0,sigma=1):\n",
    "    return 1-normal_probability_between(lo,hi,mu,sigma)\n",
    "\n",
    "def normal_upper_bound(probability, mu=0, sigma=1):\n",
    "    \"\"\"returns z for which P(Z<=z) = probability\"\"\"\n",
    "    return inverse_normal_cdf(probability, mu, sigma)\n",
    "\n",
    "def normal_lower_bound(probability, mu=0, sigma=1):\n",
    "    \"\"\"returns z for which P(Z>=z) = probability\"\"\"\n",
    "    return inverse_normal_cdf((1-probability),mu,sigma)\n",
    "\n",
    "def normal_two_sided_bound(probability, mu=0, sigma=1):\n",
    "    \"\"\"returns the symmetric (about mean) bounds\"\"\"\n",
    "    lo_probability = (1-probability)/2\n",
    "    hi_probability = probability+lo_probability\n",
    "    lower_bound    = normal_upper_bound(lo_probability,mu,sigma)\n",
    "    upper_bound    = normal_upper_bound(hi_probability,mu,sigma)\n",
    "    return lower_bound,upper_bound"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Example flipping a coin\n",
    "We want to test weather a coin is fair. So we'll make the assumption abpout probability p of coin landing Heads (0.5 for our case). So, our null hypothesis is that coin is fair- i.e. p = 0.5. We'll test this against the alternative hypothesis p != 5.\n",
    "In particular our test will involve flipping the coin *n* number of times and counting number of Heads X. Since each coin flip is a Bernoulli trial, which means X should be a Binomial(n,p) random variable. Which can be approximated as normal. In Particular we choose to flip the coin 1000 times. \n",
    "##### If our hypothesis of fairness is TRUE, X should be distributed approximately normally with mean 500 and standard deviation 15.8. \n",
    "Also, we need to make a decision about *significance* - how willing we are to make *type 1 error* (\"False positive\"), in which we reject *H0* even though it's true (let's choose 5%).\n",
    "Consider a test, where we reject *H0* if it falls outside the bounds given by 95% probability bounds. \n",
    "Assuming *p* really equals 0.5 (i.e. H0 is TRUE), there is just a 5% chance we observe an X that lies outside this interval, which is the exact significance we wanted."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mean: 500.0 std: 15.811388300841896\n",
      "95% probability bounds: (469.01026640487555, 530.9897335951244)\n"
     ]
    }
   ],
   "source": [
    "mu_0, sigma_0 = normal_approximation_to_binomial(1000,0.5)\n",
    "print(\"mean:\",mu_0, \"std:\",sigma_0)\n",
    "print(\"95% probability bounds:\", normal_two_sided_bound(0.95, mu_0, sigma_0))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Power of a test (likelihood of rejecting the null hypothesis when it's false or 1 - Type 2 error)\n",
    "<https://www.moresteam.com/whitepapers/download/power-stat-test.pdf>\n",
    "\n",
    "|     *    |   H0 is TRUE      |  H0 is FALSE                    |\n",
    "|:---------|:-----------------:|:-------------------------------:|\n",
    "|Accepted  | 95%               | Type 2 error                    |\n",
    "|Rejected  | Type 1 error (5%) | Power of test 1-P(Type 2 error) |\n",
    "\n",
    "\n",
    "To measure the probability of making a type 2 error, we have to specify what exactly H0 being false means\n",
    "(It is generally a specific condition under the alternative hypothesis)\n",
    "For this analysis assume that p = 0.55 (or the coin is slightly biased towards Heads) and check what happens."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Power: 0.89\n"
     ]
    }
   ],
   "source": [
    "# 95% bounds based on assumption p is 0.5\n",
    "lo, hi = normal_two_sided_bound(0.95, mu_0, sigma_0)\n",
    "\n",
    "# actual mu and sigma based on p=0.55\n",
    "mu_1, sigma_1 = normal_approximation_to_binomial(1000, 0.55)\n",
    "\n",
    "# a type 2 error means we fail to reject null hypothesis even when it's false.\n",
    "# which will happen when if in our original Test, X is still in our 95% confidence interval\n",
    "type_2_probability = normal_probability_between(lo,hi,mu_1,sigma_1)\n",
    "power = 1 - type_2_probability\n",
    "print(\"Power:\", round(power,2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### One tailed test\n",
    "If instead of our original Hypothesis coin is fair (p=0.5) we are interested in coin is not biased towards Head or that p<=0.5. In that case we want a *one sided test* that rejects null hypothesis when *X* is much large than 500 but not when it is less than 500. Suppose we still want significance of 5% i.e. error of rejecting null hypothesis when it's TRUE. This means we have to find the cutoff below which 95% probability lies."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "We will reject null hypothesis if observed value is above: 526.0073585242053\n",
      "Power: 0.94\n"
     ]
    }
   ],
   "source": [
    "hi = normal_upper_bound(0.95,mu_0,sigma_0)\n",
    "print(\"We will reject null hypothesis if observed value is above:\", hi)\n",
    "\n",
    "type_2_probability = normal_probability_below(hi, mu_1, sigma_1)\n",
    "power = 1 - type_2_probability\n",
    "print(\"Power:\", round(power,2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is a more powerful test than before since it no longer rejects *H0* if it's below lower bound (469) but only when it's above an upper bound (526)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### p-values\n",
    "An alternate way to think about the tests above is that instead of choosing bounds based on some probability cutoff, we compute the probability assuming *H0* is TRUE that we would see a value at least as extreme as the one actually observed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "p-value for observing 530 Heads: 0.062\n",
      "p-value from simulation: 0.061\n"
     ]
    }
   ],
   "source": [
    "# For our two-sided test of weather coin is fair, we compute:\n",
    "\n",
    "def two_sided_p_value(x, mu=0, sigma=1):\n",
    "    if x>=mu:\n",
    "        return 2*normal_probability_above(x,mu,sigma)\n",
    "    else:\n",
    "        return 2*normal_probability_below(x,mu,sigma)\n",
    "    \n",
    "# So if we were to see 530 Heads, we would compute: (taking into account continuity correction)\n",
    "p_value_530_heads = two_sided_p_value(529.5,mu_0,sigma_0)\n",
    "print(\"p-value for observing 530 Heads:\", round(p_value_530_heads,3))\n",
    "\n",
    "# Another way to estimate the p-value in this scenario is by simulation\n",
    "extreme_value_count = 0\n",
    "num_of_experiments   = 10000\n",
    "num_coin_tosses_per_experiment = 1000\n",
    "for _ in range(num_of_experiments):\n",
    "    num_heads = sum([1 if random.random()>0.5 else 0                  # count # of Heads \n",
    "                     for _ in range(num_coin_tosses_per_experiment)]) # in 1000 flips\n",
    "    if num_heads >= 530 or num_heads <= 470:                          # and count how often the # is 'extreme'\n",
    "        extreme_value_count += 1\n",
    "\n",
    "p_value_simulation = extreme_value_count/num_of_experiments\n",
    "print(\"p-value from simulation:\", round(p_value_simulation,3))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since the p value > 5% significance level so it is not rare to observe 530 Heads and we do not reject *H0*.\n",
    "If we instead observed 532 Heads, the *p-value* would be:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "p-value for observing 532 Heads: 0.046\n"
     ]
    }
   ],
   "source": [
    "p_value_532_heads = two_sided_p_value(531.5, mu_0, sigma_0)\n",
    "print(\"p-value for observing 532 Heads:\", round(p_value_532_heads,3))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Which is smaller than the 5% significane level, so it is rare to observe such a value and if observed then *H0* should be rejected."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Confidence intervals\n",
    "We've been interested in the value of heads probability p, which is a parameter of the unknown Heads distribution. We can use a third approach to construct a confidence interval around the observed value of parameter (p in our case).\n",
    "For example if 525 Heads are observed in a sample of 1000 flips, then we estimate p equals 0.525.\n",
    "How confident can we be about this estimate? From Central limit theorem we know that since these are Bernoulli variables and they should be approximately Normally distributed with mean p and standard deviation sqrt(np(1-p)/1000).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "95% confidence bounds around observed probability: (0.494, 0.556)\n"
     ]
    }
   ],
   "source": [
    "# For this case \n",
    "p_hat = 525/1000\n",
    "mu    = p_hat\n",
    "sigma = math.sqrt(p_hat*(1-p_hat)/1000)\n",
    "(lo, hi) = normal_two_sided_bound(0.95, mu, sigma)\n",
    "print(\"95% confidence bounds around observed probability:\", (round(lo,3), round(hi,3)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since *0.5* falls under the 95% confidence bounds so *H0* is not rejected"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### P-hacking\n",
    "If we are setting out to find \"significant\" results, we usually can. Test enough hypothesis against data set and one of them will most certainly appear significant. Remove right outliers, and the results can appear significant.\n",
    "This is sometimes called p-hacking and is in someway a consequence of \"inference from p-value framework\".\n",
    "https://www.quora.com/What-is-p-hacking\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "46\n"
     ]
    }
   ],
   "source": [
    "def run_experiment():\n",
    "    \"\"\"flip a fair coin 1000 times, True = Heads, False = tails\"\"\"\n",
    "    return [random.random() > 0.5 for _ in range(1000)]\n",
    "\n",
    "def reject_fairness(experiment):\n",
    "    \"\"\"using 5% significance levels and outcomes of an experiment \n",
    "        determine weather the experiment indicates if the coin is biased (not fair)\"\"\"\n",
    "    num_heads = len([flip for flip in experiment if flip])\n",
    "    return num_heads < 469 or num_heads > 531\n",
    "\n",
    "random.seed(0)\n",
    "experiments = [run_experiment() for _ in range(1000)]\n",
    "\n",
    "# Count number of experiments where fairness hypothesis os rejected with 5% significance\n",
    "num_rejections = len([experiment \n",
    "                     for experiment in experiments if reject_fairness(experiment)])\n",
    "\n",
    "print(num_rejections)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This can falsefully lead to a conclusion that the coin is biased, or in other words the results are significant. Because given enough number of experiments there will always be some results that are significant. P-hacking means using this fact to distort findings."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Running A/B test (two sample t-test really)\n",
    "Let's say we want to compare which of the 2 Ads is effective. A being shown to *Na* number of peope and *na* people click on it and B being shown to *Nb* number of people and *nb* people click on it. We need to determine if there is a significant difference between these Ads. We can treat each Ad view as a Bernoulli trial. And if *Na* is large then *na/Na* should be approximately normally distributed with mean *pa* and standard deviation *sqrt(pa(1-pa)/Na*. Same is TRUE for B."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def estimated_parameters(N,n):\n",
    "    p = n/N\n",
    "    sigma = math.sqrt(p*(1-p)/N)\n",
    "    return p,sigma\n",
    "\n",
    "def a_b_statistic(N_A, n_A, N_B, n_B):\n",
    "    p_A, sigma_A = estimated_parameters(N_A,n_A)\n",
    "    p_B, sigma_B = estimated_parameters(N_B, n_B)\n",
    "    return (p_B-p_A)/(math.sqrt(sigma_A**2+sigma_B**2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If we assume the two Normals are independent, then their difference should also be normal with mean pb - pa and standard deviation sqrt(sigma_a^2 + sigma_b^2).\n",
    "##### This may not be entirely correct since we don't know the population standard deviation and are trying to infer population standard deviation using samples, which means the a_b_statistic is essentially a t-distribution. But for large enough data sets it's close enough.\n",
    "Let's assume A gets 200/1000 clicks and B gets 180/1000 clicks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "t-statistic for B and A: -1.14\n",
      "The probability of seeing such a large difference if means were actually equal would be: 0.25\n",
      "So null hypothesis that means are equal is not rejected\n"
     ]
    }
   ],
   "source": [
    "t = a_b_statistic(1000,200,1000,180)\n",
    "print(\"t-statistic for B and A:\", round(t,2))\n",
    "p_value = round(two_sided_p_value(t),2)\n",
    "# Does not need mu and sigma as it is already a Z value\n",
    "\n",
    "print(\"The probability of seeing such a large difference if means were actually equal would be:\",p_value) \n",
    "print(\"So null hypothesis that means are equal is not rejected\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.3"
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "position": {
    "height": "556px",
    "left": "1070px",
    "right": "20px",
    "top": "120px",
    "width": "350px"
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
